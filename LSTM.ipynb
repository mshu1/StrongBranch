{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import src.graph_ops as go\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = np.load(\"data/graph_10node7500.npy\")\n",
    "# options: graph_10+20k, graph_10+10k\n",
    "labels = np.load(\"data/labels_10node7500.npy\")\n",
    "# options: labels_10+20k, labels_10+10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = go.compress_graphs(graphs)\n",
    "X = X[:,None,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(np.zeros((labels.shape[0], 10)))\n",
    "y = y.add(pd.get_dummies(labels[:,0]), fill_value=0)\n",
    "y = y.add(pd.get_dummies(labels[:,1]), fill_value=0)\n",
    "y = y.values.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(input, target, size_average=True):\n",
    "    \"\"\" Cross entropy that accepts soft targets\n",
    "    Args:\n",
    "         pred: predictions for neural network\n",
    "         targets: targets, can be soft\n",
    "         size_average: if false, sum is returned instead of mean\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        input = torch.FloatTensor([[1.1, 2.8, 1.3], [1.1, 2.1, 4.8]])\n",
    "        input = torch.autograd.Variable(out, requires_grad=True)\n",
    "\n",
    "        target = torch.FloatTensor([[0.05, 0.9, 0.05], [0.05, 0.05, 0.9]])\n",
    "        target = torch.autograd.Variable(y1)\n",
    "        loss = cross_entropy(input, target)\n",
    "        loss.backward()\n",
    "    \"\"\"\n",
    "    return torch.mean(-target * torch.log(input))\n",
    "\n",
    "\n",
    "class LSTM(nn.Module): \n",
    "    \n",
    "    #constructor\n",
    "    #take in X as a parameter\n",
    "    def __init__(self, X, y, hidden_dim = 50, num_layers = 4, weight_decay = 0.0005):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = 'cpu'\n",
    "        \n",
    "        #Find dimensionality of X, y\n",
    "        X_dim = X.shape[-1]\n",
    "        y_dim = y.shape[-1]\n",
    "        \n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(X_dim, hidden_dim, num_layers)\n",
    "        \n",
    "        #Output\n",
    "        self.linear = nn.Linear(hidden_dim, y_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "        # Define what optimization we want to use.\n",
    "        # Adam is a popular method so I'll use it.\n",
    "        # L2 regularization in weight decay.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay = weight_decay)\n",
    "        self.to(self.device)\n",
    "        \n",
    "    # 1. input X\n",
    "    def forward(self, X):\n",
    "        X, _ = self.lstm(X)\n",
    "        X = self.linear(X)\n",
    "        X = self.sigmoid(X)\n",
    "        X = 2*X/((X[:,0,:]).sum(1))[:,None,None]\n",
    "        return X\n",
    "    \n",
    "    def loss(self, pred, true):\n",
    "        #PyTorch's own cross entropy loss function.\n",
    "        score = cross_entropy\n",
    "        l = None\n",
    "        for i in range(true.shape[1]):\n",
    "            if l is None:\n",
    "                l = score(pred[:,0,i], true[:,i])\n",
    "            else:\n",
    "                l += score(pred[:,0,i], true[:,i])\n",
    "        return l\n",
    "    \n",
    "\n",
    "    def fit(self, X, y, early_stopping = True, patience = 50):\n",
    "        \n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_arr = X.values\n",
    "            X_tens = Variable(torch.Tensor(X_arr).float())\n",
    "        else:\n",
    "            X_tens = Variable(torch.Tensor(X).float())\n",
    "        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n",
    "            y_arr = y.values\n",
    "            if len(y_arr.shape) == 1:\n",
    "                y_arr = y_arr[:,None]\n",
    "            y_tens = Variable(torch.Tensor(y_arr).float())\n",
    "        else:\n",
    "            if len(y.shape) == 1:\n",
    "                y = y[:,None]\n",
    "            y_tens = Variable(torch.Tensor(y).float())\n",
    "        \n",
    "        X_tens = X_tens.to(self.device)\n",
    "        y_tens = y_tens.to(self.device)\n",
    "        \n",
    "        if early_stopping == True:\n",
    "            \n",
    "            inds = np.arange(len(X_tens))\n",
    "            num_train = int(len(X_tens) * .9)\n",
    "            train_inds = np.random.choice(inds, num_train, replace=False)\n",
    "            val_inds = np.setdiff1d(inds, train_inds)\n",
    "            X_train = X_tens[train_inds]\n",
    "            y_train = y_tens[train_inds]\n",
    "            X_val = X_tens[val_inds]\n",
    "            y_val = y_tens[val_inds]\n",
    "            \n",
    "        for epoch in range(1000):\n",
    "            self.optimizer.zero_grad()\n",
    "            if early_stopping==True:\n",
    "                out_train = self.forward(X_train)\n",
    "                loss = self.loss(out_train, y_train)\n",
    "            if early_stopping==False:\n",
    "                out_train = self.forward(X_tens)\n",
    "                loss = self.loss(out_train, y_tens)\n",
    "                print(loss)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                continue\n",
    "                \n",
    "            out_val = self.forward(X_val)\n",
    "            loss_val = self.loss(out_val, y_val)\n",
    "            print(loss_val)\n",
    "\n",
    "\n",
    "            if epoch == 0:\n",
    "                min_loss = loss_val\n",
    "                best_weights = self.state_dict()\n",
    "                counter = 0\n",
    "            else:\n",
    "                if loss_val < min_loss:\n",
    "                    min_loss = loss_val\n",
    "                    best_weights = self.state_dict()\n",
    "                    counter = 0\n",
    "                else:\n",
    "                    counter += 1\n",
    "                    if counter == patience:\n",
    "                        self.load_state_dict(best_weights)\n",
    "                        return\n",
    "                \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X_arr = X.values\n",
    "            X_tens = Variable(torch.Tensor(X_arr).float())\n",
    "        else:\n",
    "            X_tens = Variable(torch.Tensor(X).float())\n",
    "            \n",
    "        X_tens = X_tens.to(self.device)\n",
    "        \n",
    "        predict_out = self.forward(X_tens)\n",
    "        temp =  predict_out.cpu().detach().numpy()\n",
    "        temp[temp > 1-1e-6] = 1-1e-6\n",
    "        temp[temp < 1e-6]=1e-6\n",
    "        temp = temp[:,0,:]\n",
    "        return temp\n",
    "    \n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X).argsort(1)[:,-2:]\n",
    "        proba.sort(1)\n",
    "        return proba\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        y_true = y.argsort(1)[:,-2:]\n",
    "        y_true.sort(1)\n",
    "        return (y_pred == y_true).all(1).mean().item()\n",
    "    \n",
    "    def top_k_acc(self, X,y, k = 5):\n",
    "        \"\"\"Get top k accuracy for prediction. I.e., were the 2 correct nodes \n",
    "        in the top k returned. Note, k=2 is the score function.\"\"\"\n",
    "        y_pred = self.predict_proba(X)\n",
    "        y_true = y.argsort(1)[:,-2:]  \n",
    "        y_true.sort(1)\n",
    "        eqs_list = []\n",
    "        top_k = y_pred.argsort(1)[:,-k:]\n",
    "        for i in range(y_true.shape[0]):\n",
    "            row = y_true[i]\n",
    "            eqs_arr = []\n",
    "            for ele in row:\n",
    "                eqs_arr.append(np.any(ele == top_k[i]))\n",
    "            eqs_arr = np.array(eqs_arr)\n",
    "            eq = eqs_arr.all()\n",
    "            eqs_list.append(eq)\n",
    "        eqs = np.array(eqs_list)\n",
    "\n",
    "        return eqs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3.1459, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.9589, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.6716, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.4756, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.3529, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.2827, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.2286, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1937, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1755, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1677, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1693, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1813, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1953, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1995, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1898, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1743, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1603, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1526, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1575, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1626, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1661, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1666, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1640, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1597, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1520, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1501, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1492, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1496, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1470, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1469, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1516, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1523, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1510, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1480, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1494, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1498, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1474, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1487, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1497, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1473, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1468, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1490, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1493, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1489, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1488, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1491, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1484, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1485, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1486, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1483, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1482, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1481, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1478, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1479, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1476, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1477, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1475, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2.1475, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lstm.fit(X_train, y_train, early_stopping = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2703133297928837"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_acc = [lstm.top_k_acc(X_test, y_test, k = i) for i in range(2,11)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2703133297928837,\n",
       " 0.4471587891662241,\n",
       " 0.6064790228359002,\n",
       " 0.7195963887413701,\n",
       " 0.8066914498141264,\n",
       " 0.8709506107275624,\n",
       " 0.9182156133828996,\n",
       " 0.9559214020180563,\n",
       " 1.0]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Tutorial]",
   "language": "python",
   "name": "conda-env-Tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
