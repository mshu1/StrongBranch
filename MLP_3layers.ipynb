{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_3layers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "jKvjcrKTA_-F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "from torch.utils import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3yc5liH3A_-I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "graphs = np.load(\"graph_10+20k.npy\")\n",
        "# options: graph_10+20k, graph_10+10k\n",
        "labels = np.load(\"labels_10+20k.npy\")\n",
        "# options: labels_10+20k, labels_10+10k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_4jK9z90A_-K",
        "colab_type": "code",
        "outputId": "1b814478-125c-4395-8691-3dbd0975daed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        }
      },
      "cell_type": "code",
      "source": [
        "num_of_graphs = graphs.shape[0]\n",
        "num_of_nodes = graphs.shape[1]\n",
        "\n",
        "\n",
        "def compress_graphs(graphs):\n",
        "    \"\"\"Convert 3d matrix of graphs, dims = (graphs,verts,verts), \n",
        "    to 2d matrix dims = (n_graphs, (verts choose 2) - verts). \n",
        "    We can omit half of the data as the matrix of each graph is \n",
        "    symmetric as we are working with undirected graphs. We can\n",
        "    also not take any entries on the main diagonal as they will \n",
        "    always be zero as we do not allow self loops.\n",
        "    \n",
        "    \n",
        "    Args: \n",
        "        graphs : (n_graphs,verts,verts) hypermatrix of graphs.\n",
        "        \n",
        "    Returns:\n",
        "        graphs_comp : (n_graphs, (vert choose 2) - verts) matrix\n",
        "            of compressed adjancey matrices.\n",
        "        \n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    verts = graphs.shape[1]\n",
        "    graphs_comp =  graphs.T[np.triu_indices(verts, 1)].T\n",
        "    \n",
        "    return graphs_comp  \n",
        "\n",
        "# compress 2d adjacency matrix to 1d array, prepare input\n",
        "compressed_graph = np.zeros((graphs.shape[0], int(num_of_nodes*(num_of_nodes-1)/2)))\n",
        "                            \n",
        "for i in range(0,graphs.shape[0]):\n",
        "    compressed_graph[i] = compress_graphs(graphs[i])\n",
        "\n",
        "graphs_train, graphs_test, train_labels, val_labels =\\\n",
        "    train_test_split(compressed_graph, labels, test_size=0.20, random_state=42)\n",
        "\n",
        "batch_size = 64\n",
        "params = {'batch_size': batch_size,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 6}\n",
        "max_epochs = 100\n",
        "\n",
        "class Dataset(data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, list_IDs, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "  def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return num_of_graphs\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = index\n",
        "\n",
        "        # Load data and get label\n",
        "        X = Variable(torch.from_numpy(compressed_graph[ID]))\n",
        "        y = self.labels[ID]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "# Generators\n",
        "training_set = Dataset(graphs_train, labels)\n",
        "training_generator = data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = Dataset(graphs_test, labels)\n",
        "validation_generator = data.DataLoader(validation_set, **params)\n",
        "\n",
        "class CompressMatrixNetwork(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        u = int(num_of_nodes*(num_of_nodes-1)/2)\n",
        "        self.fc1 = nn.Linear(u, 2*u)\n",
        "        self.fc3 = nn.Linear(2*u, 85)\n",
        "        self.fc4 = nn.Linear(85, 40)\n",
        "        self.fc5 = nn.Linear(85, 40)\n",
        "        self.fc6 = nn.Linear(40, num_of_nodes)\n",
        "        self.fc7 = nn.Linear(40, num_of_nodes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x1 = F.relu(self.fc4(x))\n",
        "        x1 = self.fc6(x1)\n",
        "        x2 = F.relu(self.fc5(x))\n",
        "        x2 = self.fc7(x2)\n",
        "        return F.softmax(x1, dim=1), F.softmax(x2, dim=1)\n",
        "\n",
        "model = CompressMatrixNetwork()\n",
        "\n",
        "for module in model.children():\n",
        "    module.reset_parameters()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "optimizer.zero_grad()\n",
        "\n",
        "\n",
        "info = []\n",
        "# Loop over epochs\n",
        "for epoch in range(max_epochs):\n",
        "    epoch_train_acc, correct_count = 0, 0\n",
        "    epoch_val_acc = 0\n",
        "    epoch_train_loss = 0\n",
        "    epoch_val_loss=0\n",
        "    count = 0\n",
        "    \n",
        "    # Training\n",
        "    for local_batch, local_labels in training_generator:\n",
        "        # Transfer to GPU\n",
        "        if torch.cuda.is_available():\n",
        "            local_batch, local_labels = local_batch.cuda(), local_labels.cuda()\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        label_batch = local_labels.long()\n",
        "        input_batch = local_batch.float()\n",
        "\n",
        "        output_batch = model(input_batch)\n",
        "\n",
        "        train_loss = F.cross_entropy(output_batch[0], label_batch[:,0]) \\\n",
        "            + F.cross_entropy(output_batch[1], label_batch[:,1])\n",
        "        epoch_train_loss += train_loss\n",
        "\n",
        "        \n",
        "        pred1 = output_batch[0].data.max(1)[1]\n",
        "        pred2 = output_batch[1].data.max(1)[1]\n",
        "        \n",
        "        matches = (label_batch[:,0] == pred1) & (label_batch[:,1] == pred2)\n",
        "        epoch_train_acc += matches.float().mean()\n",
        "        correct_count += matches.sum()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        train_loss.backward()\n",
        "        optimizer.step()\n",
        "        count += 1\n",
        "    epoch_train_loss = epoch_train_loss / count\n",
        "    epoch_train_acc = epoch_train_acc / count\n",
        "    # Validation\n",
        "    count = 0\n",
        "    with torch.no_grad():\n",
        "        for local_batch, local_labels in validation_generator:\n",
        "            # Transfer to GPU\n",
        "            local_batch, local_labels = local_batch.cuda(), local_labels.cuda()\n",
        "\n",
        "            model.eval()\n",
        "            \n",
        "            label_batch = local_labels.long()\n",
        "            input_batch = local_batch.float()\n",
        "            \n",
        "            output_batch = model(input_batch)\n",
        "            \n",
        "            \n",
        "            val_loss = F.cross_entropy(output_batch[0], label_batch[:,0]) \\\n",
        "                + F.cross_entropy(output_batch[1], label_batch[:,1])\n",
        "            epoch_val_loss += val_loss\n",
        "\n",
        "            pred1 = output_batch[0].data.max(1)[1]\n",
        "            pred2 = output_batch[1].data.max(1)[1]\n",
        "            \n",
        "            matches = (label_batch[:,0] == pred1) & (label_batch[:,1] == pred2)\n",
        "            epoch_val_acc += matches.float().mean()\n",
        "            count += 1\n",
        "        epoch_val_loss = epoch_val_loss / count\n",
        "        epoch_val_acc = epoch_val_acc / count\n",
        "    info.append([epoch, epoch_train_loss, epoch_val_loss, epoch_train_acc, epoch_val_acc])\n",
        "    print('epoch {:5d}: Obtained a training acc of {:.3f}.'.format(epoch, epoch_train_acc))\n",
        "    print('epoch {:5d}: Obtained a validation acc of {:.3f}.'.format(epoch, epoch_val_acc))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch     0: Obtained a training acc of 0.002.\n",
            "epoch     0: Obtained a validation acc of 0.002.\n",
            "epoch     1: Obtained a training acc of 0.003.\n",
            "epoch     1: Obtained a validation acc of 0.003.\n",
            "epoch     2: Obtained a training acc of 0.003.\n",
            "epoch     2: Obtained a validation acc of 0.003.\n",
            "epoch     3: Obtained a training acc of 0.004.\n",
            "epoch     3: Obtained a validation acc of 0.004.\n",
            "epoch     4: Obtained a training acc of 0.010.\n",
            "epoch     4: Obtained a validation acc of 0.017.\n",
            "epoch     5: Obtained a training acc of 0.020.\n",
            "epoch     5: Obtained a validation acc of 0.021.\n",
            "epoch     6: Obtained a training acc of 0.021.\n",
            "epoch     6: Obtained a validation acc of 0.022.\n",
            "epoch     7: Obtained a training acc of 0.023.\n",
            "epoch     7: Obtained a validation acc of 0.026.\n",
            "epoch     8: Obtained a training acc of 0.032.\n",
            "epoch     8: Obtained a validation acc of 0.038.\n",
            "epoch     9: Obtained a training acc of 0.051.\n",
            "epoch     9: Obtained a validation acc of 0.067.\n",
            "epoch    10: Obtained a training acc of 0.087.\n",
            "epoch    10: Obtained a validation acc of 0.109.\n",
            "epoch    11: Obtained a training acc of 0.131.\n",
            "epoch    11: Obtained a validation acc of 0.152.\n",
            "epoch    12: Obtained a training acc of 0.168.\n",
            "epoch    12: Obtained a validation acc of 0.180.\n",
            "epoch    13: Obtained a training acc of 0.186.\n",
            "epoch    13: Obtained a validation acc of 0.191.\n",
            "epoch    14: Obtained a training acc of 0.194.\n",
            "epoch    14: Obtained a validation acc of 0.197.\n",
            "epoch    15: Obtained a training acc of 0.217.\n",
            "epoch    15: Obtained a validation acc of 0.248.\n",
            "epoch    16: Obtained a training acc of 0.292.\n",
            "epoch    16: Obtained a validation acc of 0.327.\n",
            "epoch    17: Obtained a training acc of 0.349.\n",
            "epoch    17: Obtained a validation acc of 0.360.\n",
            "epoch    18: Obtained a training acc of 0.364.\n",
            "epoch    18: Obtained a validation acc of 0.366.\n",
            "epoch    19: Obtained a training acc of 0.366.\n",
            "epoch    19: Obtained a validation acc of 0.366.\n",
            "epoch    20: Obtained a training acc of 0.366.\n",
            "epoch    20: Obtained a validation acc of 0.366.\n",
            "epoch    21: Obtained a training acc of 0.366.\n",
            "epoch    21: Obtained a validation acc of 0.366.\n",
            "epoch    22: Obtained a training acc of 0.366.\n",
            "epoch    22: Obtained a validation acc of 0.366.\n",
            "epoch    23: Obtained a training acc of 0.366.\n",
            "epoch    23: Obtained a validation acc of 0.366.\n",
            "epoch    24: Obtained a training acc of 0.366.\n",
            "epoch    24: Obtained a validation acc of 0.366.\n",
            "epoch    25: Obtained a training acc of 0.366.\n",
            "epoch    25: Obtained a validation acc of 0.366.\n",
            "epoch    26: Obtained a training acc of 0.366.\n",
            "epoch    26: Obtained a validation acc of 0.366.\n",
            "epoch    27: Obtained a training acc of 0.366.\n",
            "epoch    27: Obtained a validation acc of 0.366.\n",
            "epoch    28: Obtained a training acc of 0.366.\n",
            "epoch    28: Obtained a validation acc of 0.366.\n",
            "epoch    29: Obtained a training acc of 0.366.\n",
            "epoch    29: Obtained a validation acc of 0.366.\n",
            "epoch    30: Obtained a training acc of 0.366.\n",
            "epoch    30: Obtained a validation acc of 0.366.\n",
            "epoch    31: Obtained a training acc of 0.366.\n",
            "epoch    31: Obtained a validation acc of 0.366.\n",
            "epoch    32: Obtained a training acc of 0.366.\n",
            "epoch    32: Obtained a validation acc of 0.366.\n",
            "epoch    33: Obtained a training acc of 0.366.\n",
            "epoch    33: Obtained a validation acc of 0.366.\n",
            "epoch    34: Obtained a training acc of 0.366.\n",
            "epoch    34: Obtained a validation acc of 0.366.\n",
            "epoch    35: Obtained a training acc of 0.366.\n",
            "epoch    35: Obtained a validation acc of 0.366.\n",
            "epoch    36: Obtained a training acc of 0.366.\n",
            "epoch    36: Obtained a validation acc of 0.366.\n",
            "epoch    37: Obtained a training acc of 0.366.\n",
            "epoch    37: Obtained a validation acc of 0.366.\n",
            "epoch    38: Obtained a training acc of 0.366.\n",
            "epoch    38: Obtained a validation acc of 0.366.\n",
            "epoch    39: Obtained a training acc of 0.366.\n",
            "epoch    39: Obtained a validation acc of 0.366.\n",
            "epoch    40: Obtained a training acc of 0.366.\n",
            "epoch    40: Obtained a validation acc of 0.366.\n",
            "epoch    41: Obtained a training acc of 0.366.\n",
            "epoch    41: Obtained a validation acc of 0.366.\n",
            "epoch    42: Obtained a training acc of 0.366.\n",
            "epoch    42: Obtained a validation acc of 0.366.\n",
            "epoch    43: Obtained a training acc of 0.366.\n",
            "epoch    43: Obtained a validation acc of 0.366.\n",
            "epoch    44: Obtained a training acc of 0.366.\n",
            "epoch    44: Obtained a validation acc of 0.366.\n",
            "epoch    45: Obtained a training acc of 0.366.\n",
            "epoch    45: Obtained a validation acc of 0.366.\n",
            "epoch    46: Obtained a training acc of 0.366.\n",
            "epoch    46: Obtained a validation acc of 0.366.\n",
            "epoch    47: Obtained a training acc of 0.366.\n",
            "epoch    47: Obtained a validation acc of 0.366.\n",
            "epoch    48: Obtained a training acc of 0.366.\n",
            "epoch    48: Obtained a validation acc of 0.366.\n",
            "epoch    49: Obtained a training acc of 0.366.\n",
            "epoch    49: Obtained a validation acc of 0.366.\n",
            "epoch    50: Obtained a training acc of 0.366.\n",
            "epoch    50: Obtained a validation acc of 0.366.\n",
            "epoch    51: Obtained a training acc of 0.366.\n",
            "epoch    51: Obtained a validation acc of 0.366.\n",
            "epoch    52: Obtained a training acc of 0.366.\n",
            "epoch    52: Obtained a validation acc of 0.366.\n",
            "epoch    53: Obtained a training acc of 0.366.\n",
            "epoch    53: Obtained a validation acc of 0.366.\n",
            "epoch    54: Obtained a training acc of 0.366.\n",
            "epoch    54: Obtained a validation acc of 0.366.\n",
            "epoch    55: Obtained a training acc of 0.366.\n",
            "epoch    55: Obtained a validation acc of 0.366.\n",
            "epoch    56: Obtained a training acc of 0.366.\n",
            "epoch    56: Obtained a validation acc of 0.366.\n",
            "epoch    57: Obtained a training acc of 0.366.\n",
            "epoch    57: Obtained a validation acc of 0.366.\n",
            "epoch    58: Obtained a training acc of 0.366.\n",
            "epoch    58: Obtained a validation acc of 0.366.\n",
            "epoch    59: Obtained a training acc of 0.366.\n",
            "epoch    59: Obtained a validation acc of 0.366.\n",
            "epoch    60: Obtained a training acc of 0.366.\n",
            "epoch    60: Obtained a validation acc of 0.366.\n",
            "epoch    61: Obtained a training acc of 0.366.\n",
            "epoch    61: Obtained a validation acc of 0.366.\n",
            "epoch    62: Obtained a training acc of 0.366.\n",
            "epoch    62: Obtained a validation acc of 0.366.\n",
            "epoch    63: Obtained a training acc of 0.366.\n",
            "epoch    63: Obtained a validation acc of 0.366.\n",
            "epoch    64: Obtained a training acc of 0.366.\n",
            "epoch    64: Obtained a validation acc of 0.366.\n",
            "epoch    65: Obtained a training acc of 0.366.\n",
            "epoch    65: Obtained a validation acc of 0.366.\n",
            "epoch    66: Obtained a training acc of 0.366.\n",
            "epoch    66: Obtained a validation acc of 0.366.\n",
            "epoch    67: Obtained a training acc of 0.366.\n",
            "epoch    67: Obtained a validation acc of 0.366.\n",
            "epoch    68: Obtained a training acc of 0.366.\n",
            "epoch    68: Obtained a validation acc of 0.366.\n",
            "epoch    69: Obtained a training acc of 0.366.\n",
            "epoch    69: Obtained a validation acc of 0.366.\n",
            "epoch    70: Obtained a training acc of 0.366.\n",
            "epoch    70: Obtained a validation acc of 0.366.\n",
            "epoch    71: Obtained a training acc of 0.366.\n",
            "epoch    71: Obtained a validation acc of 0.366.\n",
            "epoch    72: Obtained a training acc of 0.366.\n",
            "epoch    72: Obtained a validation acc of 0.366.\n",
            "epoch    73: Obtained a training acc of 0.366.\n",
            "epoch    73: Obtained a validation acc of 0.366.\n",
            "epoch    74: Obtained a training acc of 0.366.\n",
            "epoch    74: Obtained a validation acc of 0.366.\n",
            "epoch    75: Obtained a training acc of 0.366.\n",
            "epoch    75: Obtained a validation acc of 0.366.\n",
            "epoch    76: Obtained a training acc of 0.366.\n",
            "epoch    76: Obtained a validation acc of 0.366.\n",
            "epoch    77: Obtained a training acc of 0.366.\n",
            "epoch    77: Obtained a validation acc of 0.366.\n",
            "epoch    78: Obtained a training acc of 0.366.\n",
            "epoch    78: Obtained a validation acc of 0.366.\n",
            "epoch    79: Obtained a training acc of 0.366.\n",
            "epoch    79: Obtained a validation acc of 0.366.\n",
            "epoch    80: Obtained a training acc of 0.366.\n",
            "epoch    80: Obtained a validation acc of 0.366.\n",
            "epoch    81: Obtained a training acc of 0.366.\n",
            "epoch    81: Obtained a validation acc of 0.366.\n",
            "epoch    82: Obtained a training acc of 0.366.\n",
            "epoch    82: Obtained a validation acc of 0.366.\n",
            "epoch    83: Obtained a training acc of 0.366.\n",
            "epoch    83: Obtained a validation acc of 0.366.\n",
            "epoch    84: Obtained a training acc of 0.366.\n",
            "epoch    84: Obtained a validation acc of 0.366.\n",
            "epoch    85: Obtained a training acc of 0.366.\n",
            "epoch    85: Obtained a validation acc of 0.366.\n",
            "epoch    86: Obtained a training acc of 0.366.\n",
            "epoch    86: Obtained a validation acc of 0.366.\n",
            "epoch    87: Obtained a training acc of 0.366.\n",
            "epoch    87: Obtained a validation acc of 0.366.\n",
            "epoch    88: Obtained a training acc of 0.366.\n",
            "epoch    88: Obtained a validation acc of 0.366.\n",
            "epoch    89: Obtained a training acc of 0.366.\n",
            "epoch    89: Obtained a validation acc of 0.366.\n",
            "epoch    90: Obtained a training acc of 0.366.\n",
            "epoch    90: Obtained a validation acc of 0.366.\n",
            "epoch    91: Obtained a training acc of 0.366.\n",
            "epoch    91: Obtained a validation acc of 0.366.\n",
            "epoch    92: Obtained a training acc of 0.366.\n",
            "epoch    92: Obtained a validation acc of 0.366.\n",
            "epoch    93: Obtained a training acc of 0.366.\n",
            "epoch    93: Obtained a validation acc of 0.366.\n",
            "epoch    94: Obtained a training acc of 0.366.\n",
            "epoch    94: Obtained a validation acc of 0.366.\n",
            "epoch    95: Obtained a training acc of 0.366.\n",
            "epoch    95: Obtained a validation acc of 0.366.\n",
            "epoch    96: Obtained a training acc of 0.366.\n",
            "epoch    96: Obtained a validation acc of 0.366.\n",
            "epoch    97: Obtained a training acc of 0.366.\n",
            "epoch    97: Obtained a validation acc of 0.366.\n",
            "epoch    98: Obtained a training acc of 0.366.\n",
            "epoch    98: Obtained a validation acc of 0.366.\n",
            "epoch    99: Obtained a training acc of 0.366.\n",
            "epoch    99: Obtained a validation acc of 0.366.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2bDmOMYUA_-M",
        "colab_type": "code",
        "outputId": "52456c29-293b-4740-8388-9642ad688334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "cell_type": "code",
      "source": [
        "x, y11, y12, y21, y22 = zip(*info)\n",
        "fig, ax = plt.subplots(2, 1, sharex=True)\n",
        "ax[0].plot(x, y11, x, y12)\n",
        "ax[0].legend(['Train loss', 'Val loss'])\n",
        "ax[1].plot(x, y21, x, y22)\n",
        "ax[1].legend(['Train acc', 'Val acc'])\n",
        "ax[1].set_ylim([0.0, 1.0])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 1.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}